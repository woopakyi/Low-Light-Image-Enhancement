import os
import glob
import argparse
import urllib.request
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
import torch

# Import modules
from model import ZeroDCEPlusPlus
from dataloader import device  # device defined in dataloader.py


# ==============================
# Load Zero-DCE++ Enhancement Model
# ==============================
def load_enhancement_model():
    model = ZeroDCEPlusPlus(num_iter=8).to(device)

    default_path = 'zero_dce_pp_trained_default.pth'
    final_path = 'zero_dce_pp_final.pth'

    if os.path.exists(default_path):
        checkpoint = default_path
        print(f"âœ“ Loading author's pre-trained enhancement model: {default_path}")
    elif os.path.exists(final_path):
        checkpoint = final_path
        print(f"âœ“ Loading your trained model: {final_path}")
    else:
        raise FileNotFoundError(
            "No enhancement model found!\n"
            "Please provide one of:\n"
            "  â€¢ zero_dce_pp_trained_default.pth (recommended â€“ provided by author)\n"
            "  â€¢ zero_dce_pp_final.pth (generated by train.py)"
        )

    model.load_state_dict(torch.load(checkpoint, map_location=device))
    model.eval()
    return model


# ==============================
# Load MobileSAM + Generator
# ==============================
def load_mobile_sam():
    try:
        from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator
    except ImportError:
        raise ImportError(
            "MobileSAM not installed. Run:\n"
            "pip install git+https://github.com/ChaoningZhang/MobileSAM.git"
        )

    checkpoint_url = "https://github.com/ChaoningZhang/MobileSAM/raw/master/weights/mobile_sam.pt"
    checkpoint_path = "mobile_sam.pt"

    if not os.path.exists(checkpoint_path):
        print("Downloading MobileSAM weights (~20MB)...")
        urllib.request.urlretrieve(checkpoint_url, checkpoint_path)
        print("Download complete!")

    mobile_sam = sam_model_registry["vit_t"](checkpoint=checkpoint_path)
    mobile_sam.to(device)
    mobile_sam.eval()

    mask_generator = SamAutomaticMaskGenerator(
        model=mobile_sam,
        points_per_side=48,
        pred_iou_thresh=0.92,
        stability_score_thresh=0.96,
        crop_n_layers=1,
        crop_n_points_downscale_factor=2,
        min_mask_region_area=500,
    )

    print("âœ“ MobileSAM loaded and optimized for precise central object segmentation.")
    return mask_generator


# ==============================
# Generate Clean Subject Mask
# ==============================
def generate_subject_mask(enhanced_np, mask_generator):
    image_uint8 = (enhanced_np * 255).astype(np.uint8)
    H, W = image_uint8.shape[:2]

    center_x, center_y = W // 2, H // 2
    half_w = int(W * 0.16)
    half_h = int(H * 0.16)
    central_region = np.zeros((H, W), dtype=np.uint8)
    central_region[center_y - half_h:center_y + half_h,
                   center_x - half_w:center_x + half_w] = 1

    print("\nRunning MobileSAM segmentation... (10â€“25 seconds)")
    masks = mask_generator.generate(image_uint8)

    if len(masks) == 0:
        print("No masks detected â†’ using central region as fallback.")
        return central_region

    def score_mask(m_dict):
        m = m_dict['segmentation']
        overlap_ratio = np.sum(m & central_region) / (np.sum(m) + 1e-6)
        area_ratio = m_dict['area'] / (H * W)
        return overlap_ratio * 20.0 - area_ratio * 8.0

    masks.sort(key=score_mask, reverse=True)

    selected_masks = []
    for m_dict in masks[:30]:
        score = score_mask(m_dict)
        area_ratio = m_dict['area'] / (H * W)
        if score > 4.0 and 0.015 < area_ratio < 0.08:
            selected_masks.append(m_dict['segmentation'])

    if not selected_masks:
        print("No perfect match â†’ using highest-scoring single mask.")
        subject_mask = masks[0]['segmentation'].astype(np.uint8)
    else:
        subject_mask = np.logical_or.reduce(selected_masks).astype(np.uint8)
        print(f"âœ“ Successfully merged {len(selected_masks)} clean parts into main subject.")

    # Morphological cleanup
    kernel_close = np.ones((11, 11), np.uint8)
    subject_mask = cv2.morphologyEx(subject_mask, cv2.MORPH_CLOSE, kernel_close, iterations=2)

    kernel_open = np.ones((11, 11), np.uint8)
    subject_mask = cv2.morphologyEx(subject_mask, cv2.MORPH_OPEN, kernel_open, iterations=2)

    kernel_tiny = np.ones((3, 3), np.uint8)
    subject_mask = cv2.dilate(subject_mask, kernel_tiny, iterations=1)

    print("âœ“ Ultimate clean subject mask ready â€“ only the main object! ðŸŽ¯\n")
    return subject_mask


# ==============================
# Main Execution
# ==============================
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate clean subject mask from low-light test image")
    parser.add_argument('--index', type=int, default=6,
                        help="Index of test image to process (0â€“14). Default: 6")
    args = parser.parse_args()

    # Load models
    enhancement_model = load_enhancement_model()
    mask_generator = load_mobile_sam()

    # Auto-detect test folder
    try:
        test_low_dir = glob.glob('**/eval15/low', recursive=True)[0]
    except IndexError:
        raise FileNotFoundError("Test folder 'eval15/low' not found. Make sure LOL dataset is extracted.")

    low_paths = sorted([
        os.path.join(test_low_dir, f)
        for f in os.listdir(test_low_dir)
        if f.lower().endswith(('.png', '.jpg', '.jpeg'))
    ])

    if args.index < 0 or args.index >= len(low_paths):
        raise ValueError(f"Index must be between 0 and {len(low_paths)-1}")

    low_path = low_paths[args.index]
    filename = os.path.basename(low_path)
    print(f"\nProcessing: {filename} (index {args.index})\n")

    # Load image
    low_pil = Image.open(low_path).convert('RGB')
    low_tensor = transforms.ToTensor()(low_pil).unsqueeze(0).to(device)

    # Enhance
    with torch.no_grad():
        enhanced_tensor, _ = enhancement_model(low_tensor)
        enhanced_tensor = torch.clamp(enhanced_tensor, 0.0, 1.0)

    low_np = low_tensor[0].cpu().permute(1, 2, 0).numpy()
    enhanced_np = enhanced_tensor[0].cpu().permute(1, 2, 0).numpy()

    # Generate mask
    subject_mask = generate_subject_mask(enhanced_np, mask_generator)

    # Visualize
    plt.figure(figsize=(18, 6))
    plt.subplot(1, 3, 1)
    plt.imshow(low_np)
    plt.title("Original Low-Light Input", fontsize=16, fontweight='bold')
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(enhanced_np)
    plt.title("Zero-DCE++ Enhanced Output", fontsize=16, fontweight='bold')
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(subject_mask, cmap='gray')
    plt.title("MobileSAM Mask Preview\n(white = main subject)", fontsize=16, fontweight='bold')
    plt.axis('off')

    plt.suptitle(f"Mask Generation Complete â€“ {filename}", fontsize=18, fontweight='bold')
    plt.tight_layout()
    plt.show()

    print("Done! Use this mask for spotlight, bokeh, relighting, or compositing.")